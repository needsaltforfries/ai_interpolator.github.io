<!DOCTYPE html>
<head>
    <title>Project Page</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Extending SepConv with Adversarial Learning</h1>
        <h2>Matthew Yang | Clemson University</h2>
        <h2>CPSC 8810 - AI Image Synthesis</h2>
        <div style="text-align:center; margin: 5px;">
            <a href="https://github.com/needsaltforfries/AI_Interpolator">Code link</a>
            <a href="Frame_Interpolation_by_Extending_SepConv_with_Generative_Adversarial_Networks.pdf" target="_blank">Report PDF</a>
        </div>
    </header>

    <section class="section">
        <div class="section-title">Abstract</div>
        <div class="content">
            <p style="word-wrap: break-word; width: 300px;">
                This GAN model adapts Niklaus et al.'s SepConv network to generate images and uses adversarial learning to
                train the generator. The adversarial learning should assist in creating frames as close to the inputs as possible.
                The SepConv model was chosen to act as the generator because it can transform a high-resolution image without issue.
            </p>
            <img src="generator.PNG">
            <img src="discriminator.PNG"></img></img>
        </div>
    </section>
    <section class="section">
        <div class="section-title">Methodology</div>
        <div class="content">
            <p style="word-wrap: break-word; width: 800px;">
                The network encodes image frames to extract the features then reconstructs them. 
                Skip connections assist in preserving image features.
                A program to write the video files iterates through a directory containing the video frames.
                The program takes two images in sequential order and feeds them through the generator, saving the resulting image in another directory.
                Using moviepy, the frames are taken from the original and new image directories and encoded into a video file.
                The network implementation was done using Pytorch and visualized using Jupyter Notebook. 
                A separate program to iterate through the necessary directories and image frames was done in Python.
            </p>
            <img src="training1.PNG" width="500px">
        </div>
    </section>

    <section class="section">
        <div class="section-title">Results</div>
        <div class="content">
            <p style="word-wrap: break-word; width: 600px;">
                Processing each frame can take less than a few seconds.
                However, each frame appears to be the average of frame 1 and frame 2.
                Two different ways to process each frame: breaking it into 64x64 patches or by entire frames.
            </p>
            <img src = "results1.PNG">
        </div>
        
    </section>

    <section class="section">
        <div class="section-title">Demo</div>
        <h3 style="text-align:center;">Original</h3>
        <div class="content">
            
            <video width="600" controls>
                <source src="clip_original.mp4" type="video/mp4">
            </video>
        </div>
        <h3 style="text-align:center;">Interpolated Video</h3>
        <div class="content">
            
            <video width="600" controls>
                <source src="clip2.mp4" type="video/mp4">
            </video>
        </div>
    </section>
</body>
</html>
